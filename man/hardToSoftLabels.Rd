% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hardToSoftLabels.R
\name{hardToSoftLabels}
\alias{hardToSoftLabels}
\title{Generate soft labels from hard labels}
\usage{
hardToSoftLabels(truth, mn = 0.5, var = 0.04, nclass)
}
\arguments{
\item{truth}{truth from a dataset, a vector which contains a true class of an object}

\item{mn}{mean which is required to generate probability p[i] from beta distribution}

\item{var}{variance which is required to generate probability p[i] from beta distribution}

\item{nclass}{number of class of an object}
}
\value{
A matrix where each line is a contour function {pl} of the object.
}
\description{
\code{hardToSoftLabels} returns a soft label from a hard one. The output is a matrix where each line is contour function of an instance.
}
\examples{
## using Iris dataset
data(iris);
truth <- iris[,5];
nclass <- 3;
hardToSoftLabels(iris[,5],mn=0.5,var=0.04,nclass);
}
\references{
B. Quost, T. Denoeux and S. Li. Parametric Classification with
      Soft Labels using the Evidential EM Algorithm. Linear Discriminant Analysis vs.
      Logistic Regression. Advances in Data Analysis and Classification, Vol. 11, Issue 4, pp 659-690, 2017.

      O. Kanjanatarakul, S. Kuson and T. Denoeux. An Evidential K-Nearest Neighbor Classifier
      based on Contextual Discounting. In: Destercke S., Denoeux T., Cuzzolin F., Martin A. (eds),
      Belief Functions: Theory and Applications. BELIEF 2018. Lecture Notes in Computer Science,
      vol 11069. Springer, pages 155-162, September 2018.
}
\author{
S. Mutmainah, D. Mercier, F. Pichon, S. Hachour
}
